[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "analysis_eda.html#splitting-the-data",
    "href": "analysis_eda.html#splitting-the-data",
    "title": "Exploratory data analysis",
    "section": "Splitting the data",
    "text": "Splitting the data\nWe split the data into training and test sets before exploring it further. Splitting the data at this point prevents data leakage, a phenomenon where decisions are made that bias the model to the data on hand such that it does not generalize well to future data.\nThe training set consists of 3/4 of the data, with the remaining 1/4 set aside for testing. The exploratory analysis below is based soley on the training set."
  },
  {
    "objectID": "analysis_eda.html#distribution-of-prices-by-borough",
    "href": "analysis_eda.html#distribution-of-prices-by-borough",
    "title": "Exploratory data analysis",
    "section": "Distribution of prices by borough",
    "text": "Distribution of prices by borough\nA histogram of sale prices (below) for each borough confirms that Manhattan’s sale prices are higher than other boroughs, but the number of single-family homes sold in Manhattan is very small."
  },
  {
    "objectID": "analysis_eda.html#prices-at-the-neighborhood-level",
    "href": "analysis_eda.html#prices-at-the-neighborhood-level",
    "title": "Exploratory data analysis",
    "section": "Prices at the neighborhood level",
    "text": "Prices at the neighborhood level\nNeighborhood-level median prices can give us an idea of how prices are distributed across the city at the local level.\n\n\n\n\n\nThe bubble graph above shows that there may be two (or more) distinct market segments represented in the data:\n\na “mainstream” market of neighborhoods with high sales volume and prices under $1 million, located in Queens, Staten Island, The Bronx, and parts of Brooklyn\na “high-end” market of neighborhoods with fewer sales and prices over $1 million, located in Manhattan and parts of Brooklyn and Queens\n\nThe segments are shown by the larger bubbles in the mainstream market and by the “elbow” in the graph, where the slope changes abruptly around $1 million. While this potential segmentation of the market will not be addressed in this project, it is useful to keep in mind while interpreting the model results."
  },
  {
    "objectID": "analysis_eda.html#price-vs.-the-predictors",
    "href": "analysis_eda.html#price-vs.-the-predictors",
    "title": "Exploratory data analysis",
    "section": "Price vs. the predictors",
    "text": "Price vs. the predictors\nFor brevity, just the highlights from the full data exploration and visualization are shown in this section.\n\nDate of sale\nSeasonality is a strong feature of the median sale price. According to national data, prices are lowest in the first and last months of the year, and highest during the summer months.1 This trend is reflected in our data. (The offset of the peak from the summer months to early fall likely coincides with the post-COVID “return to office” around this time.) Note that it is a nonlinear trend, which along with other nonlinear trends will influence the types of models that will be selected in the model building section.\n\n\n\n\n\n\n\n\n\n\nLot and building area\nAs expected, prices increase along with both lot size and the square footage of the building on the lot. The price between price and lot size is basically linear above a certain threshold. But as shown on the graph of price vs. lot size (below left), lots under 2500 sq ft are found across the full range of prices. Presumably, location matters for small lots.\nIn contrast, there is a strongly nonlinear relationship between price and residential area. This finding corresponds to another finding (not shown) that homes with more floors tend to sell for higher prices.\n\n\n\n\n\n\n\n\n\n\nLatitude and longitude\nWhile the relationship between price and latitude shows some distinct nonlinearity, the relationship between price and longitude is especially pronounced, with a prominent hump around the middle of the range. This hump presumably represents the more expensive neighborhoods in Manhattan and Brooklyn, while other parts of the city to the east and west have dramatically lower home prices.\n\n\n\n\n\n\n\n\n\n\nDistance to City Hall\nThe relationship between price and distance to city hall is highly nonlinear. The shape of this graph is very striking: prices fall quickly as distance increases, but level off around 10-12 km. A “shelf” at a sale price of $1,000,000 is visible here. The number of homes above that sale price is 2,054, while the number at or below that sale price is 10,763.\n\n\n\n\n\n\n\nDistance to nearest subway\nOf all the variables representing the distance to the nearest amenity, the distance to the subway has the most pronounced and clear relationship to price. This relationship is graphed on an untransformed x-axis and a log-transformed x-axis for comparison. Almost all homes over $2 million are near subway stations, and the trend is for lower prices as distance increases.\nRelationships between price and other distance amenities were similar, but less pronounced. Notably, the relationship between price and distance to parks was basically flat, and the maximum distance was fairly short – NYC parks are be plentiful and well-distributed around the city.\nThe relationships between price and the counts of amenities nearby were similar: the more amenities nearby, the higher the price tends to be. Again, parks were the exception: the relationship was flat.\n\n\n\n\n\n\n\n\n\n\nSchool quality\nVisualizing school quality yields both expected and unexpected results. Elementary school quality is positively associated with prices and almost all homes sold for $2 million and over have better elementary schools nearby. But neither of these relationships hold true for middle school quality."
  },
  {
    "objectID": "analysis_eda.html#correlation-plot",
    "href": "analysis_eda.html#correlation-plot",
    "title": "Exploratory data analysis",
    "section": "Correlation plot",
    "text": "Correlation plot\nCorrelation plot of all variables. None of the variables are correlated with each other at a level higher than 0.80 (though year built and year last altered are at 0.80). Excessive correlation (which can violate model assumptions and lead to poor results) will not be an issue in the modeling process."
  },
  {
    "objectID": "analysis_intro.html#background",
    "href": "analysis_intro.html#background",
    "title": "Introduction",
    "section": "Background",
    "text": "Background\nNew York City has an unusual housing market compared to most American cities. First, only about one-third of households own their homes1. Second, single-family homes are relatively rare in New York City – only one in eight housing units sold in 2021 were in this category.\nIn the most desirable neighborhoods in Manhattan and Brooklyn, single-family homes are mostly rowhomes, like the classic brownstone. In other parts of the city, homes in this category range from beachfront cottages to sprawling mansions. The rarity and diversity of single-family homes makes predicting their prices a challenging task. For companies in the real estate industry, a model that predicts prices accurately can be the difference between profit and going out of business.\nThe dataset compiled here incorporates information about each property, its location, and nearby services and amenities. In contrast, the Ames Iowa Housing dataset encountered in educational contexts consists mostly of the details of the home – number of bedrooms and bathrooms, features like basements and driveways, building materials etc.2 The location and amenities approach may be more useful in a city as large and geographically diverse as New York City.\n\nStatement of problem\nThe goal of the project is to build a model to predict sale prices of single-family homes in New York City using a combination of location features, areal features, and features of the property. The homes included in the training dataset should be move-in ready and sold on the open market, as those are the kinds of homes for which accurate pricing has the largest potential return for businesses."
  },
  {
    "objectID": "analysis_intro.html#summary-of-data",
    "href": "analysis_intro.html#summary-of-data",
    "title": "Introduction",
    "section": "Summary of data",
    "text": "Summary of data\nAll data are publicly available from https://opendata.cityofnewyork.us. The data has been sourced and compiled by the author.\nA few important considerations before getting into the data:\n\nAll the data used in this project are open data from city agencies, which may be less complete than commercial datasets.\nA large number of variables were explored for inclusion in the final set, but ultimately rejected. Some variables were relevant to only a small proportion of lots, while others had poor coverage or excessive missing values. Certain variables were highly correlated with the outcome, notably tax assessment data.\nScreening the homes in the dataset was an important task for this project, and could not be done automatically in R. The screening process is briefly described below.\n\n\nVariables\nOutcome: Sale price\nPredictors:\n\nLocation:\n\nBorough\nLatitude and longitude\nDistance to city hall (used as measure of centrality)\n\nProperty attributes:\n\nLot area in square feet\nResidential area in square feet\nRatio of lot area to residential area\nNumber of floors\nYear built\nYear of last major alteration\nProperty landmarked or in historic district\n\nNearby amenities\n\nDistance to nearest…\n\nsubway station\nbike route\npark\nOpen Street\n\nNumber of each of the above within walking distance\nNumber of new housing units built nearby since 2010\nSchool quality ratings for zoned elementary and middle schools"
  },
  {
    "objectID": "analysis_model.html#tidymodels",
    "href": "analysis_model.html#tidymodels",
    "title": "Building the predictive model",
    "section": "Tidymodels",
    "text": "Tidymodels\nWe will use the tidymodels framework for building and tuning the model. Tidymodels uses similar principles to the tidyverse to provide standard interfaces across many different model types. It uses an integrated workflow for all parts of the modeling process, from data preprocessing to tuning models to fitting the final model."
  },
  {
    "objectID": "analysis_model.html#selecting-one-model-from-several",
    "href": "analysis_model.html#selecting-one-model-from-several",
    "title": "Building the predictive model",
    "section": "Selecting one model from several",
    "text": "Selecting one model from several\nThe nonlinear relationships uncovered in exploratory data analysis indicate that models that capture nonlinear effects will be more suitable for our data. Additionally, given that we have more than a handful of predictors, it is more efficient to use models with automatic feature selection, which will weight variables by relevance or importance during model training. This capability will avoid the need to manually specify predictors in the model recipe.\nWe will compare the performance of these four models in order to select one for tuning.\n\nLinear model: An ordinary least squares model, included to compare to the other models and not as a candidate.\nMARS model: A regression model that captures nonlinear relationships using spline features.\nBoosted trees with xgboost: A random forest that learns from the performance of previous trees.\nRules-based model with cubist: A model that uses ensembles of rules derived from “flattened” trees.\n\n\nSpecifying the models\nAll the models are initially specified to use the default settings for their engines, rather than having settings manually specified or determined with tuning.\n\n\nCode\n# LM model spec\nlm_spec <-\n  linear_reg() |>\n  set_mode(\"regression\") |>\n  set_engine(\"lm\")\n\n# MARS model spec\nearth_spec <- \n  mars() |>\n  set_mode(\"regression\") |> \n  set_engine(\"earth\")\n\n# Boosted tree model spec\nxgboost_spec <- \n  boost_tree() |>\n  set_mode(\"regression\") |> \n  set_engine(\"xgboost\")\n\n# Cubist model spec\ncubist_spec <- \n  cubist_rules() |> \n  set_engine(\"Cubist\")\n\n\n\n\nWriting a recipe for preprocessing\nIn tidymodels, preprocessing the data is done by writing “recipes”. All models can use the same recipe, as none of them have mutually-exclusive requirements for preprocessing.\n\n\nCode\nbase_recipe <-\n  # Specify the basic recipe\n  recipe(\n    formula = sale_price ~ .,\n    data = sfh_prices_train\n  ) |>\n  # Change variable roles to identifier, rather than predictors\n  update_role(c(\n    sale_id,\n    address,\n    bbl,\n    neighborhood,\n    nta_code\n  ),\n    new_role = \"identifier\"\n  ) |>\n  # Convert the borough variable to a factor\n  step_string2factor(all_of(\"borough\")) |>\n  # Convert sale dates into week-of-year\n  step_date(\n    one_of(\"sale_date\"),\n    features = \"week\",\n    keep_original_cols = FALSE\n  ) |>\n  # Convert the historic district indicator into an explicit numeric\n  step_mutate(\n    historic = as.numeric(historic)\n  ) |>\n  # Impute any missing values\n  step_impute_knn(all_numeric_predictors()) |>\n  # Create dummy variables for the borough variable, using one-hot encoding\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>\n  # Zero-variance filter\n  step_zv(all_predictors()) |>\n  # Correlation filter\n  step_corr(\n    all_predictors(),\n    threshold = 0.9\n  ) |>\n  # Normalize (center and scale) all predictors\n  step_normalize(all_predictors())\n\n\n\n\nDefining a cross-validation scheme\nOverfitting to the training set can be mitigated by using a resampling scheme to repeatedly test models generated during the training process. Ten-fold cross-validation, used here, splits the data randomly into ten subsets and holds one in reserve to validate the model fitted on the other nine. Cross-validation is repeated 10 times, holding each subset in reserve once in turn. The performance statistics generated during each round of cross-validation are averaged to produce a final metric for evaluating the model.\n\n\nCode\n# Create a resampling scheme using 10-fold cross-validation.\nset.seed(412412)\nsfh_prices_folds <-\n  vfold_cv(\n    data = sfh_prices_train,\n    v = 10,\n    repeats = 1,\n    strata = sale_price\n  )"
  },
  {
    "objectID": "analysis_model.html#comparing-multiple-models",
    "href": "analysis_model.html#comparing-multiple-models",
    "title": "Building the predictive model",
    "section": "Comparing multiple models",
    "text": "Comparing multiple models\nAll three nonlinear models substantially outperformed the linear model. The MARS model underperformed the others by a small margin, while the Cubist and boosted tree models were very close in both RMSE and R-squared.\nThe Cubist model will be selected for tuning, as it has a slight edge over the boosted tree model in consistency (i.e., smaller standard error), though either model would work here.\n\n\n# A tibble: 8 × 4\n  model        .metric       mean    std_err\n  <chr>        <chr>        <dbl>      <dbl>\n1 linear_reg   rmse    373563.    13664.    \n2 linear_reg   rsq          0.709     0.0180\n3 mars         rmse    308800.    14761.    \n4 mars         rsq          0.799     0.0198\n5 boost_tree   rmse    276004.    13874.    \n6 boost_tree   rsq          0.840     0.0180\n7 cubist_rules rmse    279250.    10639.    \n8 cubist_rules rsq          0.839     0.0130"
  },
  {
    "objectID": "analysis_model.html#tuning-the-selected-model",
    "href": "analysis_model.html#tuning-the-selected-model",
    "title": "Building the predictive model",
    "section": "Tuning the selected model",
    "text": "Tuning the selected model\nThe Cubist model will be trained with 10 sets of these two parameters:\n\nNumber of committees: the number of iterations allowed for formulating and adjusting the rules\nNumber of neighbors: the number of nearby points used for adjusting the predictions produced by the model\n\nThe parameter sets are generated using a random grid search method."
  },
  {
    "objectID": "analysis_model.html#tuning-results",
    "href": "analysis_model.html#tuning-results",
    "title": "Building the predictive model",
    "section": "Tuning results",
    "text": "Tuning results\nThe tuning results show that all candidate parameter sets outperformed the untuned model, some by about as 15%.\n\n\n# A tibble: 10 × 5\n   model        .metric    mean     n std_err\n   <chr>        <chr>     <dbl> <int>   <dbl>\n 1 cubist_rules rmse    244874.    10   7784.\n 2 cubist_rules rmse    244971.    10   6963.\n 3 cubist_rules rmse    245265.    10   7546.\n 4 cubist_rules rmse    245285.    10   7327.\n 5 cubist_rules rmse    245903.    10   7666.\n 6 cubist_rules rmse    249281.    10   6899.\n 7 cubist_rules rmse    252045.    10   6317.\n 8 cubist_rules rmse    253730.    10   9640.\n 9 cubist_rules rmse    256380.    10   9190.\n10 cubist_rules rmse    269865.    10   6236.\n\n\nWe select a model with an algorithm that favors a less complex model (in this case, fewer committees), allowing a performance loss of up to than 2% compared the best-performing model. The principle here is that a less complex model has a better chance of generalizing to the test set.\nThe model selected by this process has the following parameters:\n\n\n# A tibble: 1 × 8\n  committees neighbors .metric .estimator    mean std_err   .best .loss\n       <int>     <int> <chr>   <chr>        <dbl>   <dbl>   <dbl> <dbl>\n1         15         8 rmse    standard   245265.   7546. 244874. 0.159\n\n\nNote that the selected Cubist model performs substantially better than the untuned model, and has only 0.76% performance loss compared to the best candidate model produced by tuning.\nHere are the selected model’s performance metrics:\n\n\n# A tibble: 2 × 4\n  model        .metric       mean    std_err\n  <chr>        <chr>        <dbl>      <dbl>\n1 cubist_rules rmse    245265.    7546.     \n2 cubist_rules rsq          0.876    0.00892"
  },
  {
    "objectID": "analysis_model.html#testing-the-tuned-model",
    "href": "analysis_model.html#testing-the-tuned-model",
    "title": "Building the predictive model",
    "section": "Testing the tuned model",
    "text": "Testing the tuned model\nThe last step is to fit the selected model to the test set. The metrics for the test fit show that the model’s performance is comparable to the best model trained on the training set.\n\n\n# A tibble: 3 × 4\n  .metric .estimator  .estimate .config             \n  <chr>   <chr>           <dbl> <chr>               \n1 rmse    standard   247730.    Preprocessor1_Model1\n2 mae     standard   117576.    Preprocessor1_Model1\n3 rsq     standard        0.869 Preprocessor1_Model1\n\n\n\nVariable importance\nBased on the variable importance plot for the fitted model, location (both absolute and in relationship to city hall), building and lot area, are the most important variables. This finding makes intuitive sense, as location and size of homes are the most heavily-promoted details in real estate listings. Of all the measures of nearby amenities, distance to the nearest subway station is the most important.\n\n\n\n\n\n\n\n\n\n\nPredictions and residuals\nThe prediction-residual plot shows significant heteroskedacity. That is, the residuals tend to increase in magnitude as predicted prices increase, indicating a non-constant variance in the residuals. The distribution in the “mainstream” segment identified during visualization (under $1 million) is compact and symmetrical, while the spread in the “high-end” segment is much wider and has notable outliers. This heteroskedacity may indicate that there are latent “rules” for the high-end housing market that do not match the median market. If this is the case, further rounds of modeling could be able to account for these rules.\nThe heteroskedacity may also be the result of a sales dataset that is relatively sparse in the high end – the number of homes sold for more than $1 million was only 2054, and only 445 homes were sold for more than $2 million.\n\n\n\n\n\n\n\n\nThe graph on the left shows the ratio of residuals to predicted prices. The graph shows that the distribution is approximately symmetrical (in this graph, a ). The near-zero slope of the trend line indicates that the model’s error bias (toward either over- or under-prediction) is consistent across the price distribution.\nThe graph on the right is the absolute-value version of the graph on the left. The mean absolute residual (deviation from perfect prediction) is shown by the blue line, and the red trend line indicates that the model tends to produce predictions with larger relative errors as prices increase – perhaps, again, due to sparse data."
  },
  {
    "objectID": "analysis_outro.html#discussion",
    "href": "analysis_outro.html#discussion",
    "title": "Discussion and next steps",
    "section": "Discussion",
    "text": "Discussion\nThe predictions from this model are reasonably good. Considering the range of prices included in the model, an RMSE of $259k and a mean absolute error of $122k are reasonable. The model performs especially well in the most dense regions of the dataset, with properties from $400k to $1.5 million.\nPredicting home prices from a combination of property attributes and location and area attributes has been shown to be a reasonably good method. The nearby amenities have not contributed much individually to the model, with the location and home size predictors being far more important. But the importance plot shows that they do collectively contribute to better performance. Refining this approach will likely yield useful results."
  },
  {
    "objectID": "analysis_outro.html#next-steps",
    "href": "analysis_outro.html#next-steps",
    "title": "Discussion and next steps",
    "section": "Next steps",
    "text": "Next steps\nRecommendations for the next steps in developing this model:\n\nExpand the dataset by incorporating more variables from commercial datasets and other open sources.\nAssess whether dividing the full range of sale prices into multiple market segments and creating models for each segment produces better results.\nCreate standardized methods for removing homes sold under market value from the dataset.\nIterate on the model building process to improve performance, tune other models besides the Cubist model, and use more intensive tuning methods to find any potential better-performing models."
  },
  {
    "objectID": "analysis_summary.html#project-goals",
    "href": "analysis_summary.html#project-goals",
    "title": "Summary",
    "section": "Project goals",
    "text": "Project goals\nBuild a model to predict sale prices of single-family homes in New York City. The data used to train the model should be publicly-available and should incorporate information about the features of each home and its lot, the location of each home, and the amenities nearby each home."
  },
  {
    "objectID": "analysis_summary.html#wrangling-the-data",
    "href": "analysis_summary.html#wrangling-the-data",
    "title": "Summary",
    "section": "Wrangling the data",
    "text": "Wrangling the data\nHome sales data was screened and filtered after import to ensure that only arms-length sales of move-in-ready homes were included in the final dataset. The range of sales prices was capped at $10 million to prevent a handful of ultra-high-priced homes from influencing the model.\nThe final dataset was compiled from datasets from several city agencies. The second major component (after sales data from the Department of Finance) was lot-level data from the Department of City Planning. Spatial methods from the sf package were used to measure the distance from each home sold to the nearest subway station, bike route, park, and open street, using data from the appropriate agencies. The count of those same amenities within walking distance of each home was also calculated. Local housing construction data and local school quality data for each home sold rounded out the final dataset."
  },
  {
    "objectID": "analysis_summary.html#exploring-the-data",
    "href": "analysis_summary.html#exploring-the-data",
    "title": "Summary",
    "section": "Exploring the data",
    "text": "Exploring the data\nThe ggplot2 package was used to make numerous visualizations during data exploration. Some linear relationships were uncovered, as were several strongly nonlinear relationships between the predictors and the outcome variable (the home sale price). None of the predictors were strongly correlated with the outcome."
  },
  {
    "objectID": "analysis_summary.html#training-and-testing-the-model",
    "href": "analysis_summary.html#training-and-testing-the-model",
    "title": "Summary",
    "section": "Training and testing the model",
    "text": "Training and testing the model\nThe model was built using packages in the tidymodels framework. Several model types were trained using a 10-fold cross-validation scheme and compared to one another. A Cubist rules-based model was chosen because it performed better and more consistently than the other models. The Cubist model was tuned using a 10-point random grid search, and a model was selected to test."
  },
  {
    "objectID": "analysis_summary.html#conclusions",
    "href": "analysis_summary.html#conclusions",
    "title": "Summary",
    "section": "Conclusions",
    "text": "Conclusions\nThe selected Cubist model performed well on the test, approximating the level of performance that was achieved during training. Overall, the model was slightly biased toward overpredicting the high end of the price range, and underpredicting the lower end of the range. Additional rounds of model development could be done to remedy this systematic bias, but the overall performance was adequate for a first round."
  },
  {
    "objectID": "analysis_wrangling1.html#importing-the-home-sales-data",
    "href": "analysis_wrangling1.html#importing-the-home-sales-data",
    "title": "Wrangling the sales data",
    "section": "Importing the home sales data",
    "text": "Importing the home sales data\nThe first major data component is the NYC Department of Finance data on all property sales for 2021. This data is composed of five Excel spreadsheets, one for each borough.1 The spreadsheets have a common structure, so writing a function to import and select columns from them cuts down on redundant code.\n\n\nCode\nread_sales_data <- function(path) {\n  readxl::read_xlsx(\n    # Specify path\n    path = path,\n    # Specify tidy column names\n    col_names = c(\n      \"borough_num\",\n      \"neighborhood\",\n      \"bldg_class_cat\",\n      \"tax_class_now\",\n      \"block\",\n      \"lot\",\n      \"easement\",\n      \"bldg_class_now\",\n      \"address\",\n      \"apt\",\n      \"zip\",\n      \"res_unit_sale\",\n      \"comm_unit_sale\",\n      \"total_unit_sale\",\n      \"land_sq_ft\",\n      \"gross_sq_ft\",\n      \"year_built_sale\",\n      \"tax_class_sale\",\n      \"bldg_class_sale\",\n      \"sale_price\",\n      \"sale_date\"\n    ),\n    # Skip header\n    skip = 8\n  ) |>\n    dplyr::filter(\n      # Filter for single-family home sales...\n      bldg_class_cat == \"01 ONE FAMILY DWELLINGS\" &\n      # ...with a single unit sold...\n      total_unit_sale == 1\n    )\n}\n\n\nAfter importing all five spreadsheets, we compile the data from all five boroughs into a single table.\nAn important next step is to create a Borough-Block-Lot (BBL) identifier for each row by combining the separate elements in the sales data. The BBL system is the city government’s method of uniquely identifying each lot in the entire city. We will use the BBL IDs to compile the data from the various sources that are available.\nEach unique sale is identified with a new column consisting of BBL plus date.\n\nErrors in the data\nThe sales data has several problems. First, there are several types of errors that must be screened for:\n\nMisclassified lots (condos in multi-unit buildings marked as single-family homes, empty lots classified as homes etc.)\nMissing information\nInaccurate information\n\nSales with these types of errors are excluded.\nSecond, there are a small number of houses that do not appear to be in habitable condition, and are sold for far less than market price. These houses were all discovered during the screening process described below, and are also excluded."
  },
  {
    "objectID": "analysis_wrangling1.html#filtering-the-sales-data",
    "href": "analysis_wrangling1.html#filtering-the-sales-data",
    "title": "Wrangling the sales data",
    "section": "Filtering the sales data",
    "text": "Filtering the sales data\n\nSweetheart deals and transfers\nThe third type of problem with the data, and by far the largest, is that there are a large number of sales that are clearly not sold in arms-length sales on the open market. Including these sales in the dataset degrades the predictive power of any model attempting to predict market prices, so they should be excluded.\n\n\n\nNote the huge number of 0-dollar sales on a histogram of all single-family home prices (below).\n\n\n\n\n\nThe data clearly contains no-cost transfers and sweetheart deals (homes sold for $5000 or even $10) between family, friends, business entities, and so on.\n\n\nFiltering out non-arms-length sales\nThe market-rate prices we are predicting more specifically described as arm’s-length sales – sales where both parties are trying to maximize their advantage in the sale. In real estate, there are a number of different kinds of non-arms-length sales:\n\nAuctions\nBank-owned properties\nShort sales\nCash-for-homes services\n\nThese sales and others should be excluded from the dataset.\nWhile it is impossible to positively identify every sale in this category, we can create a filter to identify candidates. To do this, we use data scraped from live listings on Zillow2 to develop an understanding of what actual market sales look like.\n\n\n\n\n\nThe distribution of single-family homes listed for sale on Zillow looks similar to that of our sales data, though with a “thicker” upper tail.\nThe Zillow data allows us to answer two important questions:\n\nWhat is the minimum one can expect to pay for a house anywhere in the city? The sales for $5000 are obviously deals of some kind, but what about $100k sales? Based on the Zillow data, any house sold for under $250,000 is almost certainly not an arm’s-length sale and should be excluded.\nWhat is the range of prices one can expect to pay for a house in a given neighborhood? Prices are not uniform across the city, so the filter should be responsive to local sub-markets. Based on the Zillow data, for any given neighborhood, houses that are sold for less than 45% of the neighborhood median sale price are likely sales that should be excluded.\n\nWe build a filter that catches all homes under $250k and all homes that were sold for less than 45% of the median price in the local area. Two measures of “local area” are used – ZIP code and neighborhood – and produce somewhat different, yet overlapping, sets of results.\nScreening the filter results reveals that a small number of sales that appear to be on-market have been picked up by the filter, but overall the filter is reasonably effective.\n\n\nScreening sales manually\nIt is reasonable to assume that the filter did not catch all homes sold off-market. The next task is to manually screen a portion of the remaining sales by searching for the address on Zillow and other websites. Over time, an intuition of what counts as a non-arms-length sale can be developed. Broadly speaking, homes that have one or more of the following attributes are excluded:\n\nListed as an auction, short sale, bank-owned or real-estate-owned property, in foreclosure etc.\nListed with language indicating a fixer-upper or other problems\nShowing visible signs of deterioration or abandonment\nSold for substantially under for-sale or recently-sold homes in the area\nSold for less than a previously-recorded sale\n\nThe screening process is time-intensive, so not every home can be screened. The focus was put on homes under $500k, homes that stood out in some way during data exploration, and homes that were identified as over- or under-predictions at some point during the model building process. Some random screening of homes was done to counter some of the biases inherent in this method of screening. About 1000 sales were screened in this manner, and 450 sales were excluded based on the results.\n\n\nCapping the distribution\nAnother important filter for the data involves excluding homes over a certain price. The statistical reason for doing this is so that ultra-high-end outliers do not exert undue leverage during the modeling process. More conceptually, at some point in the price distribution, homes become out of reach of everyone except the ultra-wealthy, and thus the “market” for these homes is conceptually distinct from the market for homes sold for $500,000 to $1,000,000.\nFor this project, the prices will be capped at $10 million, a 40x multiple of the minimum price included. The decision to cap the prices here is ultimately arbitrary and may be higher than is truly useful – the principle of not discarding potentially useful data was important to the choice of where to cap.\n\n\n\n\n\nThe distribution of the screened and filtered sale price variable is similar in shape to both the original data and the Zillow data, meaning no extreme distortions were introduced during the process. An artifact visible in this graph that we can call the “million-dollar shelf” will show up more clearly in later graphs."
  },
  {
    "objectID": "analysis_wrangling2.html#pluto-data",
    "href": "analysis_wrangling2.html#pluto-data",
    "title": "Wrangling the other data",
    "section": "PLUTO data",
    "text": "PLUTO data\nThe next step is to import data from PLUTO files. “Primary Land Use Tax Lot Output”1 is the name for publicly available data for each tax lot (identified by a BBL ID) in the city. It contains location data, district data, and features of the buildings on the tax lot, like floor area and the year it was built. All of the features of the properties that were not found in the sales data are found here.\n\n\nCode\npluto_data <-\n  read_csv(\n    file = \"../data/pluto/Primary_Land_Use_Tax_Lot_Output__PLUTO_.csv\",\n    # Guess column type based on more rows than default\n    guess_max = 120000\n  ) |>\n  filter(bbl %in% sales_all_boroughs_filter_cap$bbl) |>\n  # Select columns and create tidy column names\n  select(\n    borough,\n    bbl,\n    address_pluto = address,\n    ct2010 = `census tract 2010`,\n    cb2010,\n    community_dist = `community board`,\n    council_dist = `council district`,\n    res_area = resarea,\n    num_floors = numfloors,\n    year_built_pluto = yearbuilt,\n    year_altered_1 = yearalter1,\n    year_altered_2 = yearalter2,\n    historic_dist = histdist,\n    landmark,\n    latitude,\n    longitude\n  )\n\n\nIn addition to variables directly from the data, three more variables are created here:\n\nFlag for historic properties: the property is a designated landmark or is part of a historic district\nThe ratio of lot size to residential area\nYear last altered: the year when the property last had major alterations that change its assessed tax value. If no alterations are recorded, the year built is used instead."
  },
  {
    "objectID": "analysis_wrangling2.html#neighborhood-tabulation-areas",
    "href": "analysis_wrangling2.html#neighborhood-tabulation-areas",
    "title": "Wrangling the other data",
    "section": "Neighborhood Tabulation Areas",
    "text": "Neighborhood Tabulation Areas\nNeighborhood Tabulation Areas2 are defined by the Department of Planning and are roughly equivalent to neighborhoods as commonly understood by city residents and those in the real estate industry. The NTA codes will be used as a short, convenient neighborhood identifier."
  },
  {
    "objectID": "analysis_wrangling2.html#distance-and-count-for-nearby-amenities",
    "href": "analysis_wrangling2.html#distance-and-count-for-nearby-amenities",
    "title": "Wrangling the other data",
    "section": "Distance-and-count for nearby amenities",
    "text": "Distance-and-count for nearby amenities\nTwo important aspects of nearby amenities are incorporated in this project: the distance to the nearest of the type, and the count of that type of amenity within walking distance. “Walking distance” is variously defined as 400m (1/4 mile) and 800m (1/2 mile), but for this project, we’ll split the difference and define it as 600m.\nTo quickly calculate distance-to-nearest and count-in-radius, this function will used.\n\n\nCode\ndist_and_count <- function(to_location, count_within_dist_m) {\n  # Returns row number of nearest feature to each BBL from nearest_to table\n  nearest_feature =\n    # Use bbl_sf every time\n    bbl_sf |>\n    st_nearest_feature(\n      y = to_location\n    )\n  \n  dist_to_loc_tbl = \n    # Use bbl_sf every time\n    bbl_sf |>\n    # Find distance to location and round\n    mutate(\n      dist_to_loc = \n        st_distance(\n          x = bbl_sf,\n          y = to_location[nearest_feature, ],\n          by_element = TRUE\n        ) |>\n        as.numeric() |>\n        round()\n    ) |>\n    # Create a tibble with BBL and corresponding distance\n    as_tibble() |>\n    select(\n      bbl,\n      dist_to_loc\n    )\n  \n  count_within_dist <-\n    # Use bbl_sf\n    bbl_sf |>\n    # Find whether each BBL is within dist_m meters of each object\n    st_is_within_distance(\n      y = to_location,\n      dist = count_within_dist_m,\n      sparse = FALSE\n    ) |>\n    # Sum across each row to find number TRUE for each BBL\n    rowSums() |>\n    # Convert to tibble\n    as_tibble_col(column_name = \"num_within_dist\")\n  \n  dist_and_count <-\n    count_within_dist |>\n    mutate(\n      # Add BBL IDs\n      bbl = dist_to_loc_tbl$bbl,\n      # Add distance to nearest\n      dist_to_nearest = dist_to_loc_tbl$dist_to_loc,\n      .before = num_within_dist\n    )\n  \n  return(\n    dist_and_count |>\n      distinct()\n  )\n}\n\n\nHere are the amenities used in this project.\n\nSubway stations3\nThe subway is vitally important for connecting far-flung neighborhoods to the central areas of the city (and to each other). But large parts of the city have no subway connections, which may lead to lower desirability and thus lower prices. A “subway station” here is defined as a series of parallel platforms that one or more subway services may use. For example, Union Square in Manhattan counts as three stations (4-5-6, L, N-Q-R-W).\n\n\nBike routes4\nBike routes are increasingly important to getting around New York City, with a “bike boom” occurring in 2020. They are attractive amenities, much like the subway. We can expect housing prices near bike routes to be higher on average, increasing with bike route density.\nBike routes are represented as a series of segments, with roughly one block per segment. The number-within-walking distance represents bike route density, all segments of all routes within 600 meters.\n\n\nParks5\nParks are synonymous with city living, offering public green space for all residents. Good nearby parks may be associated with higher home prices. Park types included here are neighborhood parks, playgrounds, and major parks. Greenstreets features, neighborhood gardens, beaches, and other kinds of minor parks are not included.\n\n\nOpen Streets6\nOpen Streets7 are a recent innovation in New York City: public streets that have been closed to motor vehicles as a way to increase the public space available to neighborhood residents. Much like parks, nearby Open Streets may be associated with higher home prices. Open Streets include any area designated an open street at any point in 2021."
  },
  {
    "objectID": "analysis_wrangling2.html#distance-to-city-hall",
    "href": "analysis_wrangling2.html#distance-to-city-hall",
    "title": "Wrangling the other data",
    "section": "Distance to City Hall",
    "text": "Distance to City Hall\nCity Hall is a good proxy for the center of the city, even though it is not geographically the center point. It is adjacent to the very wealthy areas of FiDi and Lower Manhattan, and the area has been a center of civic and commercial life for centuries."
  },
  {
    "objectID": "analysis_wrangling2.html#local-housing-construction",
    "href": "analysis_wrangling2.html#local-housing-construction",
    "title": "Wrangling the other data",
    "section": "Local housing construction",
    "text": "Local housing construction\nDepartment of Buildings data for completed jobs since 2010.8 Completed housing jobs are included since there are two competing popular intuitions: housing construction either drives prices up through gentrification, or drives prices down by increasing the housing supply.\n“Housing jobs” can be new construction and alteration (adding units) and demolition (removing units). The net impact of these jobs is summed for each census tract, a small area that is a collection of adjacent blocks."
  },
  {
    "objectID": "analysis_wrangling2.html#school-quality",
    "href": "analysis_wrangling2.html#school-quality",
    "title": "Wrangling the other data",
    "section": "School quality",
    "text": "School quality\nSchool quality is often seen as exerting a strong influence on housing prices, with the intuition being that higher prices are associated with better schools. In NYC, elementary and middle schools each have their own zone systems, while high schools operate on a competitive admissions system similar to colleges. Because high schoolers will only sometimes attend schools near their place of residence, only elementary and middle schools are included in the data.\nThe School Quality Guide9 data is used, specifically the Parental Quality survey data. The survey consists of 6 quality metrics that are averaged to obtain an overall quality score for each school. Then all schools serving a zone have that score averaged. Finally, the zone for each lot is found and the score from that zone assigned to the lot."
  },
  {
    "objectID": "analysis_wrangling2.html#final-dataset",
    "href": "analysis_wrangling2.html#final-dataset",
    "title": "Wrangling the other data",
    "section": "Final dataset",
    "text": "Final dataset\nAfter compiling all the above sources, we have the final dataset.\n\n\nCode\nsfh_prices <-\n  all_data |>\n  select(\n    # Identifiers\n    sale_id,\n    address,\n    bbl,\n    borough,\n    neighborhood,\n    nta_code,\n    # Outcome variable\n    sale_price,\n    # Predictor variables\n    sale_date,\n    land_sq_ft,\n    gross_sq_ft,\n    lot_to_res_ratio,\n    num_floors,\n    year_built = year_built_sale,\n    year_last_altered,\n    historic,\n    latitude,\n    longitude,\n    dist_to_city_hall,\n    dist_to_subway,\n    num_subway_600m,\n    dist_to_bike_route,\n    num_bike_route_600m,\n    dist_to_park,\n    num_parks_600m,\n    dist_to_open_street,\n    num_open_streets_600m,\n    net_units_tract,\n    schools_quality_elem,\n    schools_quality_middle\n  )"
  },
  {
    "objectID": "index.html#major-projects",
    "href": "index.html#major-projects",
    "title": "Welcome to my portfolio!",
    "section": "Major projects",
    "text": "Major projects\nPredicting NYC Housing Prices\nMapping NYC Housing Prices"
  },
  {
    "objectID": "leaflet_maps_condos_2021.html",
    "href": "leaflet_maps_condos_2021.html",
    "title": "Mapping condo sales in 2021",
    "section": "",
    "text": "Sales by census tract"
  },
  {
    "objectID": "leaflet_maps_coops_2021.html",
    "href": "leaflet_maps_coops_2021.html",
    "title": "Mapping co-op apartment sales in 2021",
    "section": "",
    "text": "Sales by census tract"
  },
  {
    "objectID": "leaflet_maps_sfh_2021.html",
    "href": "leaflet_maps_sfh_2021.html",
    "title": "Mapping single-family home sales in 2021",
    "section": "",
    "text": "Sales by census tract\n\n\n\n\n\n\n\n\nIndividual sales"
  }
]